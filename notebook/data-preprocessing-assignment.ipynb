{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d991f544",
   "metadata": {
    "papermill": {
     "duration": 0.002841,
     "end_time": "2025-09-10T15:55:31.819283",
     "exception": false,
     "start_time": "2025-09-10T15:55:31.816442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "The dataset requires cleaning to ensure consistency and remove noise.  \n",
    "Steps include:\n",
    "\n",
    "- **Handling missing & invalid data**: remove rows with empty text or invalid labels.  \n",
    "- **Duplicate & conflicting data removal**: drop exact, normalized, and conflicting duplicates.  \n",
    "- **Text normalization**:  \n",
    "  - Lowercasing  \n",
    "  - Removing URLs and @mentions  \n",
    "  - Keeping hashtag words (`#happy → happy`)  \n",
    "  - Expanding contractions (`can't → can not`)  \n",
    "  - Mapping emoticons (`:) → smile`)  \n",
    "  - De-elongation (`soooo → soo`)  \n",
    "  - Replacing numbers with `<num>`  \n",
    "  - Preserving `!` and `?` as emotion cues  \n",
    "- **Stopword removal**: removes common words (`the, is, at`) but keeps negations (`no, not, never`).  \n",
    "- **Lemmatization**: reduces words to base form (`running → run`, `better → good`).  \n",
    "- **Short/empty text removal**: discards samples with fewer than 2 tokens.  \n",
    "- **Optional filters**: language filtering (keep English only), near-duplicate removal using TF-IDF cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662a97c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T15:55:31.826552Z",
     "iopub.status.busy": "2025-09-10T15:55:31.826225Z",
     "iopub.status.idle": "2025-09-10T15:55:39.802478Z",
     "shell.execute_reply": "2025-09-10T15:55:39.801216Z"
    },
    "papermill": {
     "duration": 7.981226,
     "end_time": "2025-09-10T15:55:39.804070",
     "exception": false,
     "start_time": "2025-09-10T15:55:31.822844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\r\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\r\n",
      "Building wheels for collected packages: langdetect\r\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=f67ddc0857ef93f4298e2529174ccbf1be307704425272b265ed7b55dbcff6f7\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\r\n",
      "Successfully built langdetect\r\n",
      "Installing collected packages: langdetect\r\n",
      "Successfully installed langdetect-1.0.9\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb54b0e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-10T15:55:39.811583Z",
     "iopub.status.busy": "2025-09-10T15:55:39.811269Z",
     "iopub.status.idle": "2025-09-10T16:31:53.131212Z",
     "shell.execute_reply": "2025-09-10T16:31:53.129988Z"
    },
    "papermill": {
     "duration": 2173.326308,
     "end_time": "2025-09-10T16:31:53.133054",
     "exception": false,
     "start_time": "2025-09-10T15:55:39.806746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 416809 → after hygiene: 416809 rows\n",
      "Language filter: kept 387970/416809 English rows\n",
      "Exact dupes removed: 364\n",
      "Normalized dupes removed: 7259\n",
      "Conflicting-label rows removed: 41159\n",
      "Short/empty rows removed: 60\n",
      "Near-duplicate pruning (TF-IDF + cosine)…\n",
      "Near-duplicate pairs: 130 | dropped 102 (from sample=60000)\n",
      "\n",
      "Saved cleaned dataset → emotions_clean.csv\n",
      "Final shape: (339128, 4)\n",
      "Label counts:\n",
      " label\n",
      "0    103607\n",
      "1    119244\n",
      "2     22871\n",
      "3     47685\n",
      "4     36523\n",
      "5      9198\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === Data Preprocessing Script (matches your documentation end-to-end) ===\n",
    "import re, html, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "INPUT_CSV  = \"/kaggle/input/emotions/text.csv\"   # change if needed\n",
    "OUTPUT_CSV = \"emotions_clean.csv\"\n",
    "VALID_LABELS = {0,1,2,3,4,5}\n",
    "\n",
    "# Feature toggles\n",
    "ENABLE_LANG_FILTER = True        # English-only (uses langdetect); auto-disables if not installed\n",
    "ENABLE_NEAR_DUP    = True        # TF-IDF + cosine near-duplicate pruning (sampled)\n",
    "ENABLE_STOPWORDS   = True        # remove stopwords BUT keep negations (no/not/never/without)\n",
    "ENABLE_LEMMATIZE   = True        # lemmatization with POS tags\n",
    "\n",
    "# Near-duplicate config\n",
    "NEAR_DUP_SAMPLE     = 60000      # sample size for TF-IDF near-dup check\n",
    "NEAR_DUP_SIMILARITY = 0.96       # cosine similarity >= this → considered near-duplicate\n",
    "\n",
    "# ---------------- Safe imports ----------------\n",
    "def _safe_import(pkg):\n",
    "    try:\n",
    "        return __import__(pkg)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "langdetect = _safe_import(\"langdetect\")\n",
    "sklearn_ok = _safe_import(\"sklearn\") is not None\n",
    "\n",
    "if ENABLE_LANG_FILTER and langdetect is None:\n",
    "    print(\"langdetect not installed → disabling language filter (pip install langdetect to enable).\")\n",
    "    ENABLE_LANG_FILTER = False\n",
    "\n",
    "# ---------------- NLTK setup for stopwords & lemma ----------------\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)\n",
    "# robust tagger (either resource name works depending on NLTK version)\n",
    "try:\n",
    "    nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "    _TAGGER_RES = \"averaged_perceptron_tagger_eng\"\n",
    "except Exception:\n",
    "    nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "    _TAGGER_RES = \"averaged_perceptron_tagger\"\n",
    "\n",
    "# Stopwords (keep negations)\n",
    "BASE_STOPWORDS = set(stopwords.words(\"english\"))\n",
    "KEEP_NEG = {\"no\", \"not\", \"never\", \"without\"}\n",
    "FINAL_STOPWORDS = BASE_STOPWORDS - KEEP_NEG\n",
    "\n",
    "# ---------------- Normalization helpers ----------------\n",
    "CONTRACTIONS = {\n",
    "    \"can't\":\"can not\", \"cant\":\"can not\", \"won't\":\"will not\", \"wont\":\"will not\",\n",
    "    \"don't\":\"do not\", \"dont\":\"do not\", \"isn't\":\"is not\", \"isnt\":\"is not\",\n",
    "    \"aren't\":\"are not\", \"arent\":\"are not\", \"doesn't\":\"does not\", \"doesnt\":\"does not\",\n",
    "    \"didn't\":\"did not\", \"didnt\":\"did not\", \"haven't\":\"have not\", \"havent\":\"have not\",\n",
    "    \"hasn't\":\"has not\", \"hasnt\":\"has not\", \"hadn't\":\"had not\", \"hadnt\":\"had not\",\n",
    "    \"i'm\":\"i am\", \"im\":\"i am\", \"it's\":\"it is\", \"he's\":\"he is\", \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\", \"there's\":\"there is\", \"what's\":\"what is\", \"who's\":\"who is\",\n",
    "    \"i've\":\"i have\", \"we've\":\"we have\", \"they've\":\"they have\",\n",
    "    \"i'll\":\"i will\", \"we'll\":\"we will\", \"you'll\":\"you will\", \"they'll\":\"they will\",\n",
    "    \"i'd\":\"i would\", \"you'd\":\"you would\", \"he'd\":\"he would\", \"she'd\":\"she would\", \"they'd\":\"they would\",\n",
    "    \"y'all\":\"you all\", \"should've\":\"should have\", \"could've\":\"could have\", \"would've\":\"would have\"\n",
    "}\n",
    "def expand_contractions(t: str) -> str:\n",
    "    keys = sorted(CONTRACTIONS.keys(), key=len, reverse=True)\n",
    "    pat = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, keys)) + r\")\\b\")\n",
    "    return pat.sub(lambda m: CONTRACTIONS[m.group(0)], t)\n",
    "\n",
    "# Emoticons mapping → words (covers the examples in your doc)\n",
    "EMOTICONS = {\n",
    "    r\":-\\)\": \"smile\", r\":\\)\": \"smile\",\n",
    "    r\":-D\": \"laugh\",  r\":D\": \"laugh\",\n",
    "    r\":-\\(\": \"sad\",   r\":\\(\": \"sad\",\n",
    "    r\";-\\)\": \"wink\",  r\";\\)\": \"wink\",\n",
    "    r\":'\\(\": \"cry\",\n",
    "    r\":-P\": \"playful\", r\":P\": \"playful\",\n",
    "    r\":-O\": \"surprise\", r\":O\": \"surprise\",\n",
    "    r\":/\": \"skeptical\", r\":-\\|\": \"neutral\"\n",
    "}\n",
    "EMOTICON_REGEX = [(re.compile(k), v) for k, v in EMOTICONS.items()]\n",
    "def replace_emoticons(text: str) -> str:\n",
    "    for rx, word in EMOTICON_REGEX:\n",
    "        text = rx.sub(f\" {word} \", text)\n",
    "    return text\n",
    "\n",
    "# Regex helpers\n",
    "URL_PATTERN       = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
    "MENTION_PATTERN   = re.compile(r\"@\\w+\")\n",
    "HASHTAG_PATTERN   = re.compile(r\"#(\\w+)\")\n",
    "MULTISPACE        = re.compile(r\"\\s+\")\n",
    "REPEAT_CHARS      = re.compile(r\"(.)\\1{2,}\")           # 3+ same character → 2\n",
    "NUM_PATTERN       = re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "def strip_punct_keep_emotion(s: str) -> str:\n",
    "    # Keep ! and ? (emotion cues), allow <num> token\n",
    "    return re.sub(r\"[^\\w\\s!?<>]\", \" \", s)\n",
    "\n",
    "# POS → wordnet mapping\n",
    "def wn_pos(tag: str):\n",
    "    c = tag[0].upper() if tag else \"N\"\n",
    "    if c == \"J\": return wordnet.ADJ\n",
    "    if c == \"V\": return wordnet.VERB\n",
    "    if c == \"N\": return wordnet.NOUN\n",
    "    if c == \"R\": return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "LEMMA = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    \"\"\"Full normalization to match documentation.\"\"\"\n",
    "    if not isinstance(txt, str):\n",
    "        return \"\"\n",
    "    # Core normalization\n",
    "    t = unicodedata.normalize(\"NFKC\", txt)\n",
    "    t = html.unescape(t)\n",
    "    t = t.strip().lower()\n",
    "    t = URL_PATTERN.sub(\" \", t)              # remove URLs\n",
    "    t = MENTION_PATTERN.sub(\" \", t)          # remove @mentions\n",
    "    t = HASHTAG_PATTERN.sub(r\"\\1\", t)        # #happy -> happy\n",
    "    t = expand_contractions(t)               # can't->can not, etc.\n",
    "    t = replace_emoticons(t)                 # :) -> smile, etc.\n",
    "    t = REPEAT_CHARS.sub(r\"\\1\\1\", t)         # sooo -> soo\n",
    "    t = NUM_PATTERN.sub(\" <num> \", t)        # numbers -> <num>\n",
    "    t = strip_punct_keep_emotion(t)          # keep ! and ?\n",
    "    t = MULTISPACE.sub(\" \", t).strip()\n",
    "\n",
    "    # Token-level: stopwords (keep negations) + lemmatization with POS\n",
    "    tokens = t.split()\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    tagged = pos_tag(tokens, lang=\"eng\")     # tag once per text\n",
    "    out = []\n",
    "    for w, tag in tagged:\n",
    "        if w in {\"!\", \"?\", \"<num>\"}:         # preserve emotion markers and <num>\n",
    "            out.append(w); continue\n",
    "        if ENABLE_STOPWORDS and (w in FINAL_STOPWORDS):\n",
    "            continue\n",
    "        if ENABLE_LEMMATIZE:\n",
    "            w = LEMMA.lemmatize(w, wn_pos(tag))\n",
    "        out.append(w)\n",
    "    return \" \".join(out)\n",
    "\n",
    "# For diagnostics & near-dup: more aggressive normalization (ignore !, ?)\n",
    "def normalize_for_dedup(t: str) -> str:\n",
    "    t = t.replace(\"!\", \" \").replace(\"?\", \" \")\n",
    "    t = re.sub(r\"[^a-z0-9\\s<>]\", \" \", t)     # keep letters, digits, <num>\n",
    "    return MULTISPACE.sub(\" \", t).strip()\n",
    "\n",
    "# ---------------- Load ----------------\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Handling missing/invalid\n",
    "n0 = len(df)\n",
    "df = df.dropna(subset=[\"text\", \"label\"])\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df = df[df[\"label\"].isin(VALID_LABELS)].copy()\n",
    "print(f\"Loaded: {n0} → after hygiene: {len(df)} rows\")\n",
    "\n",
    "# (Optional) language filter\n",
    "if ENABLE_LANG_FILTER:\n",
    "    from langdetect import detect\n",
    "    def is_en(s):\n",
    "        try: return detect(s) == \"en\"\n",
    "        except Exception: return False\n",
    "    df[\"__is_en\"] = df[\"text\"].map(is_en)\n",
    "    kept = int(df[\"__is_en\"].sum())\n",
    "    print(f\"Language filter: kept {kept}/{len(df)} English rows\")\n",
    "    df = df[df[\"__is_en\"]].drop(columns=\"__is_en\")\n",
    "\n",
    "# --- Exact duplicates (same raw text & label) ---\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"text\", \"label\"])\n",
    "print(f\"Exact dupes removed: {before - len(df)}\")\n",
    "\n",
    "# --- Text normalization ---\n",
    "df[\"clean_text\"] = df[\"text\"].map(clean_text)\n",
    "\n",
    "# --- Normalized duplicates (same clean_text & label) ---\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"clean_text\", \"label\"])\n",
    "print(f\"Normalized dupes removed: {before - len(df)}\")\n",
    "\n",
    "# --- Conflicting duplicates (same clean_text with different labels → drop all) ---\n",
    "counts_by_clean = df.groupby(\"clean_text\")[\"label\"].nunique()\n",
    "conflict_keys = set(counts_by_clean[counts_by_clean > 1].index)\n",
    "before = len(df)\n",
    "if conflict_keys:\n",
    "    df = df[~df[\"clean_text\"].isin(conflict_keys)].copy()\n",
    "    print(f\"Conflicting-label rows removed: {before - len(df)}\")\n",
    "else:\n",
    "    print(\"No conflicting-label duplicates.\")\n",
    "\n",
    "# --- Very short/empty after cleaning (fewer than 2 tokens) ---\n",
    "token_lens = df[\"clean_text\"].str.split().map(len)\n",
    "before = len(df)\n",
    "df = df[(df[\"clean_text\"] != \"\") & (token_lens >= 2)].copy()\n",
    "print(f\"Short/empty rows removed: {before - len(df)}\")\n",
    "\n",
    "# --- norm_text (for reporting/near-dup) ---\n",
    "df[\"norm_text\"] = df[\"clean_text\"].map(normalize_for_dedup)\n",
    "\n",
    "# --- Optional: near-duplicate pruning via TF-IDF + cosine on a sample ---\n",
    "if ENABLE_NEAR_DUP and sklearn_ok and len(df) > 2000:\n",
    "    print(\"Near-duplicate pruning (TF-IDF + cosine)…\")\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    sub = df.sample(n=min(NEAR_DUP_SAMPLE, len(df)), random_state=42).copy()\n",
    "    vec = TfidfVectorizer(min_df=2, max_df=0.98)\n",
    "    X = vec.fit_transform(sub[\"norm_text\"])\n",
    "\n",
    "    try:\n",
    "        nn = NearestNeighbors(metric=\"cosine\", n_neighbors=5, n_jobs=-1)\n",
    "    except TypeError:  # sklearn>=1.4 removed n_jobs in some estimators\n",
    "        nn = NearestNeighbors(metric=\"cosine\", n_neighbors=5)\n",
    "    nn.fit(X)\n",
    "    distances, indices = nn.kneighbors(X, return_distance=True)\n",
    "\n",
    "    threshold = 1.0 - NEAR_DUP_SIMILARITY\n",
    "    pairs = set()\n",
    "    for i, (dists, nbrs) in enumerate(zip(distances, indices)):\n",
    "        for dist, j in zip(dists[1:], nbrs[1:]):  # skip self\n",
    "            if dist <= threshold:\n",
    "                pairs.add(tuple(sorted((i, j))))\n",
    "    to_drop = {b for (_, b) in pairs}\n",
    "    before_s = len(sub)\n",
    "    sub = sub.drop(index=sub.index[list(to_drop)]).copy()\n",
    "    print(f\"Near-duplicate pairs: {len(pairs)} | dropped {before_s - len(sub)} (from sample={before_s})\")\n",
    "\n",
    "    # Merge back: keep deduped sample, keep the rest\n",
    "    df = pd.concat([df.drop(index=sub.index, errors=\"ignore\"), sub], axis=0)\n",
    "\n",
    "# ---------------- Save & report ----------------\n",
    "cols = [\"text\", \"label\", \"clean_text\", \"norm_text\"]\n",
    "df[cols].to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\nSaved cleaned dataset → {OUTPUT_CSV}\")\n",
    "print(\"Final shape:\", df.shape)\n",
    "print(\"Label counts:\\n\", df[\"label\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b288388",
   "metadata": {
    "papermill": {
     "duration": 0.002764,
     "end_time": "2025-09-10T16:31:53.139066",
     "exception": false,
     "start_time": "2025-09-10T16:31:53.136302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Feature Extraction (TF-IDF)\n",
    "\n",
    "Once the dataset is cleaned, the text is transformed into numerical features  \n",
    "using **Term Frequency–Inverse Document Frequency (TF-IDF)**.\n",
    "\n",
    "Configuration:\n",
    "- **n-grams (1,2):** unigrams (single words) + bigrams (two-word phrases).  \n",
    "- **min_df = 2:** discard words appearing only once.  \n",
    "- **sublinear_tf = True:** log-scaled term frequency.  \n",
    "- **max_features = 80,000:** limit vocabulary size for efficiency.  \n",
    "\n",
    "We fit TF-IDF on the **training set only** (to avoid data leakage),  \n",
    "then transform validation and test sets with the same vocabulary.  \n",
    "Finally, the features are saved in `.npz` format for re-use in different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8101589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T16:31:53.146361Z",
     "iopub.status.busy": "2025-09-10T16:31:53.146027Z",
     "iopub.status.idle": "2025-09-10T16:32:15.453928Z",
     "shell.execute_reply": "2025-09-10T16:32:15.452717Z"
    },
    "papermill": {
     "duration": 22.313808,
     "end_time": "2025-09-10T16:32:15.455792",
     "exception": false,
     "start_time": "2025-09-10T16:31:53.141984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: X_*_tfidf.npz, y_*.npy, tfidf_vectorizer.joblib\n",
      "Train shape: (237389, 80000) Val shape: (50869, 80000) Test shape: (50870, 80000)\n"
     ]
    }
   ],
   "source": [
    "# === TF-IDF feature extraction (fit once, save) ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "DATA_PATH = OUTPUT_CSV\n",
    "RAND = 42\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "X = df[\"clean_text\"].astype(str)\n",
    "y = df[\"label\"].astype(int)\n",
    "\n",
    "# Split (70/15/15)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=RAND\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RAND\n",
    ")\n",
    "\n",
    "# Fit TF-IDF on train only\n",
    "vec = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2,\n",
    "    sublinear_tf=True,\n",
    "    max_features=80000\n",
    ")\n",
    "Xtr = vec.fit_transform(X_train)\n",
    "Xva = vec.transform(X_val)\n",
    "Xte = vec.transform(X_test)\n",
    "\n",
    "# Save features + labels + vectorizer\n",
    "sparse.save_npz(\"X_train_tfidf.npz\", Xtr)\n",
    "sparse.save_npz(\"X_val_tfidf.npz\",   Xva)\n",
    "sparse.save_npz(\"X_test_tfidf.npz\",  Xte)\n",
    "np.save(\"y_train.npy\", y_train.values)\n",
    "np.save(\"y_val.npy\",   y_val.values)\n",
    "np.save(\"y_test.npy\",  y_test.values)\n",
    "\n",
    "joblib.dump(vec, \"tfidf_vectorizer.joblib\")\n",
    "\n",
    "print(\"Saved: X_*_tfidf.npz, y_*.npy, tfidf_vectorizer.joblib\")\n",
    "print(\"Train shape:\", Xtr.shape, \"Val shape:\", Xva.shape, \"Test shape:\", Xte.shape)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4403839,
     "sourceId": 7563141,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2210.585749,
   "end_time": "2025-09-10T16:32:16.986584",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-10T15:55:26.400835",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
